{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PoisonDkNN_FinalNotebook.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyNZueRX1gvb7Lot2ybKpVEP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Bharys/IFT6756_H2021_Project/blob/main/PoisonDkNN_FinalNotebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xMVmD6UymmJA"
      },
      "source": [
        "To create Poison Samples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sG0WLv0FWd8p"
      },
      "source": [
        "%%capture --no-stderr --no-display\n",
        "\n",
        "try:\n",
        "  import secml\n",
        "except ImportError:\n",
        "  %pip install git+https://gitlab.com/secml/secml"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aeGS_L-EmyCG",
        "outputId": "fcd60133-002a-42c1-d320-d487c3720771"
      },
      "source": [
        "!pip install numpy==1.16.1"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: numpy==1.16.1 in /usr/local/lib/python3.7/dist-packages (1.16.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_lCoSQiBm4wa"
      },
      "source": [
        "from secml.data.loader import CDataLoaderMNIST\n",
        "\n",
        "loader = CDataLoaderMNIST()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UBU1lbvThHOG"
      },
      "source": [
        "random_state = 999\n",
        "\n",
        "n_tr = 100  # Number of training set samples\n",
        "n_val = 500  # Number of validation set samples\n",
        "n_ts = 500  # Number of test set samples\n",
        "\n",
        "digits = (1, 9)\n",
        "\n",
        "tr_val = loader.load('training', digits=digits, num_samples=n_tr + n_val)\n",
        "ts = loader.load('testing', digits=digits, num_samples=n_ts)\n",
        "\n",
        "# Split in training and validation set\n",
        "tr = tr_val[:n_tr, :]\n",
        "val = tr_val[n_tr:, :]\n",
        "\n",
        "# Normalize the features in `[0, 1]`\n",
        "tr.X /= 255\n",
        "val.X /= 255\n",
        "ts.X /= 255\n",
        "\n",
        "from secml.ml.classifiers import CClassifierSVM\n",
        "# train SVM in the dual space, on a linear kernel, as needed for poisoning\n",
        "clf = CClassifierSVM(C=10, kernel='linear')\n",
        "\n",
        "print(\"Training of classifier...\")\n",
        "clf.fit(tr.X, tr.Y)\n",
        "\n",
        "# Compute predictions on a test set\n",
        "y_pred = clf.predict(ts.X)\n",
        "\n",
        "# Metric to use for performance evaluation\n",
        "from secml.ml.peval.metrics import CMetricAccuracy\n",
        "metric = CMetricAccuracy()\n",
        "\n",
        "# Evaluate the accuracy of the classifier\n",
        "acc = metric.performance_score(y_true=ts.Y, y_pred=y_pred)\n",
        "\n",
        "print(\"Accuracy on test set: {:.2%}\".format(acc))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7CqYtQ6AnC7C"
      },
      "source": [
        "from secml.figure import CFigure\n",
        "# Only required for visualization in notebooks\n",
        "%matplotlib inline\n",
        "\n",
        "# Let's define a convenience function to easily plot the MNIST dataset\n",
        "def show_digits(samples, preds, labels, digs, n_display=50):\n",
        "    samples = samples.atleast_2d()\n",
        "    print('i am sample',samples.shape[0])\n",
        "    n_display = min(n_display, samples.shape[0])\n",
        "    fig = CFigure(width=n_display*2, height=3)\n",
        "    for idx in range(n_display):\n",
        "        fig.subplot(2, n_display, idx+1)\n",
        "        fig.sp.xticks([])\n",
        "        fig.sp.yticks([])\n",
        "        fig.sp.imshow(samples[idx, :].reshape((28, 28)), cmap='gray')\n",
        "        fig.sp.title(\"{} ({})\".format(digits[labels[idx].item()], digs[preds[idx].item()]),\n",
        "                     color=(\"green\" if labels[idx].item()==preds[idx].item() else \"red\"))\n",
        "        '''\n",
        "        if(preds[idx].item()==1):\n",
        "          name_fig = '/content/1/'+str(digits[preds[idx].item()])+'_'+str(idx)+'.png'\n",
        "        else:\n",
        "          name_fig = '/content/9/'+str(digs[preds[idx].item()])+'_'+str(idx)+'.png'\n",
        "        fig.savefig(name_fig)\n",
        "        '''\n",
        "        \n",
        "    fig.show()\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "YlNCH81gnFnW",
        "outputId": "bdd097a6-1e02-43a9-a6b6-7c8843481fbe"
      },
      "source": [
        "lb, ub = 0., 1.  # Bounds of the attack space. Can be set to `None` for unbounded\n",
        "n_poisoning_points = 50 # Number of poisoning points to generate\n",
        "\n",
        "# Should be chosen depending on the optimization problem\n",
        "solver_params = {\n",
        "    'eta': 0.25,\n",
        "    'eta_min': 2.0,\n",
        "    'eta_max': None,\n",
        "    'max_iter': 100,\n",
        "    'eps': 1e-6\n",
        "}\n",
        "\n",
        "from secml.adv.attacks import CAttackPoisoningSVM\n",
        "pois_attack = CAttackPoisoningSVM(classifier=clf,\n",
        "                                  training_data=tr,\n",
        "                                  val=val,\n",
        "                                  lb=lb, ub=ub,\n",
        "                                  solver_params=solver_params,\n",
        "                                  random_seed=random_state)\n",
        "pois_attack.n_points = n_poisoning_points\n",
        "\n",
        "# Run the poisoning attack\n",
        "print(\"Attack started...\")\n",
        "\n",
        "pois_y_pred, _, pois_points_ds, _ = pois_attack.run(ts.X, ts.Y)\n",
        "print(\"Attack complete!\")\n",
        "\n",
        "# Evaluate the accuracy of the original classifier\n",
        "acc = metric.performance_score(y_true=ts.Y, y_pred=clf.predict(ts.X))\n",
        "# Evaluate the accuracy after the poisoning attack\n",
        "pois_acc = metric.performance_score(y_true=ts.Y, y_pred=pois_y_pred)\n",
        "print((y_pred))\n",
        "print(\"Original accuracy on test set: {:.2%}\".format(acc))\n",
        "print(\"Accuracy after attack on test set: {:.2%}\".format(pois_acc))\n",
        "\n",
        "# Training of the poisoned classifier for visualization purposes\n",
        "pois_clf = clf.deepcopy()\n",
        "pois_tr = tr.append(pois_points_ds)  # Join the training set with the poisoning points\n",
        "pois_clf.fit(pois_tr.X, pois_tr.Y)\n",
        "\n",
        "show_digits(pois_points_ds.X, pois_clf.predict(pois_points_ds.X), \n",
        "            pois_points_ds.Y, digits)\n",
        "\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Attack started...\n",
            "2021-04-30 06:37:03,300 - root - ERROR - Internal Python error in the inspect module.\n",
            "Below is the traceback from this internal error.\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-6-01765323f026>\", line 25, in <module>\n",
            "    pois_y_pred, _, pois_points_ds, _ = pois_attack.run(ts.X, ts.Y)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/secml/adv/attacks/poisoning/c_attack_poisoning.py\", line 555, in run\n",
            "    xc[idx, :] = self._run(xc, yc, idx=idx)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/secml/adv/attacks/poisoning/c_attack_poisoning.py\", line 476, in _run\n",
            "    x = self._solver.maximize(self._x0)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/secml/optim/optimizers/c_optimizer.py\", line 194, in maximize\n",
            "    x = self.minimize(x_init, args=args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/secml/optim/optimizers/c_optimizer_pgd_ls.py\", line 322, in minimize\n",
            "    x, fx = self._xk(x, fx, *args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/secml/optim/optimizers/c_optimizer_pgd_ls.py\", line 229, in _xk\n",
            "    z, fz = self._line_search.minimize(x, -grad, fx)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/secml/optim/optimizers/line_search/c_line_search_bisect.py\", line 336, in minimize\n",
            "    eta_max = self._compute_eta_max(x, d, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/secml/optim/optimizers/line_search/c_line_search_bisect.py\", line 229, in _compute_eta_max\n",
            "    z = self._update_z(x, eta, d)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/secml/optim/optimizers/line_search/c_line_search_bisect.py\", line 115, in _update_z\n",
            "    self._fz = self.fun.fun(z)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/secml/optim/function/c_function.py\", line 106, in fun\n",
            "    out_fun = self._fun(x, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/secml/optim/optimizers/c_optimizer.py\", line 184, in fun_inv\n",
            "    return -wrapped_fun(z, *f_args, **f_kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/secml/optim/function/c_function.py\", line 106, in fun\n",
            "    out_fun = self._fun(x, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/secml/adv/attacks/poisoning/c_attack_poisoning.py\", line 371, in objective_function\n",
            "    clf, tr = self._update_poisoned_clf()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/secml/adv/attacks/poisoning/c_attack_poisoning.py\", line 338, in _update_poisoned_clf\n",
            "    self._poisoned_clf.fit(tr.X, tr.Y)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/secml/ml/classifiers/c_classifier.py\", line 140, in fit\n",
            "    return super(CClassifier, self).fit(x, y)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/secml/ml/c_module.py\", line 283, in fit\n",
            "    return self._fit(x, y)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/secml/ml/classifiers/sklearn/c_classifier_svm.py\", line 239, in _fit\n",
            "    self._fit_binary(x, y, svc_kernel)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/secml/ml/classifiers/sklearn/c_classifier_svm.py\", line 268, in _fit_binary\n",
            "    svc.fit(x.tondarray(), y.get_data())\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py\", line 199, in fit\n",
            "    fit(X, y, sample_weight, solver_type, kernel, random_seed=seed)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py\", line 258, in _dense_fit\n",
            "    max_iter=self.max_iter, random_seed=random_seed)\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\", line 1823, in showtraceback\n",
            "    stb = value._render_traceback_()\n",
            "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/ultratb.py\", line 1132, in get_records\n",
            "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/ultratb.py\", line 313, in wrapped\n",
            "    return f(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/ultratb.py\", line 358, in _fixed_getinnerframes\n",
            "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
            "  File \"/usr/lib/python3.7/inspect.py\", line 1502, in getinnerframes\n",
            "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
            "  File \"/usr/lib/python3.7/inspect.py\", line 1460, in getframeinfo\n",
            "    filename = getsourcefile(frame) or getfile(frame)\n",
            "  File \"/usr/lib/python3.7/inspect.py\", line 696, in getsourcefile\n",
            "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
            "  File \"/usr/lib/python3.7/inspect.py\", line 742, in getmodule\n",
            "    os.path.realpath(f)] = module.__name__\n",
            "  File \"/usr/lib/python3.7/posixpath.py\", line 395, in realpath\n",
            "    path, ok = _joinrealpath(filename[:0], filename, {})\n",
            "  File \"/usr/lib/python3.7/posixpath.py\", line 429, in _joinrealpath\n",
            "    if not islink(newpath):\n",
            "  File \"/usr/lib/python3.7/posixpath.py\", line 171, in islink\n",
            "    st = os.lstat(path)\n",
            "KeyboardInterrupt\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w_5U9NK-nVAD"
      },
      "source": [
        "import numpy as np\n",
        "a = pois_clf.predict(pois_points_ds.X)\n",
        "a = a.tondarray((50,))\n",
        "\n",
        "a = np.where(a==1,9,1)\n",
        "print(a)\n",
        "b = pois_points_ds.X.tondarray((50,784))\n",
        "np.save('poisoned_data_Y', a)\n",
        "np.save('poisoned_data_X', b)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lALF4Ht9nXs2"
      },
      "source": [
        "x = np.load('poisoned_data_X.npy')\n",
        "y = np.load('poisoned_data_Y.npy')\n",
        "\n",
        "samples = pois_points_ds.X.atleast_2d()\n",
        "print(samples.shape)\n",
        "'''for i in range(samples.shape[0]):\n",
        "  for j in range(samples.shape[1]):\n",
        "    print(int(samples[i,j]))\n",
        "'''\n",
        "#narr = samples.tondarray((15,784))\n",
        "\n",
        "print(x)\n",
        "print(y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LW8yY0n9naKL"
      },
      "source": [
        "Poison Datasamples creation ends here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1QmvBvQynfQN"
      },
      "source": [
        "DkNN Code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x0Dwk7FInhJS"
      },
      "source": [
        "! apt-get install libomp-dev\n",
        "\n",
        "! pip uninstall torch torchvision -y\n",
        "! pip install torch==1.4.0 torchvision==0.5.0\n",
        "! pip install falconn==1.3.1\n",
        "! pip install faiss==1.5.3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8xEiSwOrnpp4"
      },
      "source": [
        "%reload_ext autoreload\n",
        "%autoreload 2\n",
        "%matplotlib inline\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, TensorDataset, DataLoader\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle\n",
        "import numpy as np\n",
        "import falconn\n",
        "import faiss\n",
        "import platform\n",
        "import enum\n",
        "import copy\n",
        "from bisect import bisect_left\n",
        "import warnings\n",
        "\n",
        "print('python version:      {}'.format(platform.python_version()))\n",
        "print('torch version:       {}'.format(torch.__version__))\n",
        "print('torchvision version: {}'.format(torchvision.__version__))\n",
        "print('numpy version:       {}'.format(np.__version__))\n",
        "print('matplotlib version:  {}'.format(matplotlib.__version__))\n",
        "print('pickle version:      {}'.format(pickle.format_version))\n",
        "\n",
        "use_cuda = torch.cuda.is_available()\n",
        "print('CUDA available:      {}'.format(use_cuda))\n",
        "print('cuDNN enabled:       {}'.format(torch.backends.cudnn.enabled))\n",
        "print('num gpus:            {}'.format(torch.cuda.device_count()))\n",
        "\n",
        "if use_cuda:\n",
        "    print('gpu:                 {}'.format(torch.cuda.get_device_name(0)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "osXVoJKYnvj8"
      },
      "source": [
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "random_seed = 0\n",
        "torch.manual_seed(random_seed)\n",
        "np.random.seed(random_seed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zjjAqqGJnvgb"
      },
      "source": [
        "num_epochs = 6            # number of training epochs\n",
        "batch_size_train = 500    # batch size for training\n",
        "batch_size_test = 1000    # batch size for testing\n",
        "learning_rate = 0.001     # learning rate for training\n",
        "calibset_size = 750       # size of the calibration set for DkNN\n",
        "neighbors = 75            # number of nearest neighbors for DkNN\n",
        "number_bits = 17          # number of bits for LSH for DkNN\n",
        "\n",
        "log_interval = 10         # printing training statistics after 10 iterations"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_HBhKz9on2gK"
      },
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,))\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sNVM7QNBn3ZQ"
      },
      "source": [
        "# Training set\n",
        "trainset = torchvision.datasets.MNIST(\n",
        "    root='./data', train=True, download=True, transform=transform\n",
        ")\n",
        "\n",
        "# Test set and calibration set\n",
        "orig_testset = torchvision.datasets.MNIST(\n",
        "    root='./data', train=False, download=True, transform=transform\n",
        ")\n",
        "orig_testset_size = len(orig_testset)\n",
        "\n",
        "testset_size = orig_testset_size - calibset_size\n",
        "testset, calibset = torch.utils.data.random_split(orig_testset, [testset_size, calibset_size])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YRJQL6rqn5fj"
      },
      "source": [
        "# Create training data loader\n",
        "trainloader = torch.utils.data.DataLoader(\n",
        "    trainset, batch_size=batch_size_train, shuffle=True, num_workers=2\n",
        ")\n",
        "\n",
        "# Create test data loader\n",
        "testloader = torch.utils.data.DataLoader(\n",
        "    testset, batch_size=batch_size_test, shuffle=False, num_workers=2\n",
        ")\n",
        "\n",
        "# Create calib data loader\n",
        "calibloader = torch.utils.data.DataLoader(\n",
        "    calibset, batch_size=calibset_size, shuffle=False, num_workers=2\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NtetJ5dnn9Pb"
      },
      "source": [
        "print('trainset size: {}'.format(len(trainloader.dataset)))\n",
        "print('testset size:  {}'.format(len(testloader.dataset)))\n",
        "print('calibset size: {}'.format(len(calibloader.dataset)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dWWrqFgRn_Nu"
      },
      "source": [
        "def show_samples(data, targets):\n",
        "    data = data.numpy()\n",
        "    print(\"tensor shape: \" + str(data.shape))\n",
        "    \n",
        "    fig = plt.figure()\n",
        "    for i in range(9):\n",
        "        plt.subplot(3,3,i+1)\n",
        "        plt.tight_layout()\n",
        "        \n",
        "        img = data * 0.3081 + 0.1307  # unnormalize\n",
        "        plt.imshow(img[i][0], cmap='gray', interpolation='none')\n",
        "        plt.title(\"Ground Truth: {}\".format(targets[i]))\n",
        "        \n",
        "        plt.xticks([])\n",
        "        plt.yticks([])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "khkTY_lMoA34"
      },
      "source": [
        "dataiter = enumerate(testloader)\n",
        "_, (sample_data, sample_targets) = next(dataiter)\n",
        "\n",
        "show_samples(sample_data, sample_targets)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i0sn-HzyoDej"
      },
      "source": [
        "def train(num_epochs, model, optim, crit, train_loader, test_loader):\n",
        "    train_losses = []\n",
        "    test_losses = []\n",
        "    test_accs = []\n",
        "    \n",
        "    for epoch in range(1, num_epochs + 1):\n",
        "        model, train_losses = train_one_epoch(epoch, model, optim, crit, train_loader, train_losses)\n",
        "        test_losses, test_accs = test(model, crit, test_loader, test_losses, test_accs)\n",
        "        \n",
        "    print('Finished Training')\n",
        "    return train_losses, test_losses, test_accs\n",
        "    \n",
        "\n",
        "def train_one_epoch(epoch_num, model, optim, crit, data_loader, losses):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    \n",
        "    for batch_idx, data in enumerate(data_loader):\n",
        "        # Get the inputs; data is a list of [inputs, labels]\n",
        "        inputs, targets = data\n",
        "\n",
        "        if use_cuda:\n",
        "            inputs, targets = inputs.cuda(), targets.cuda()\n",
        "        \n",
        "        # Zero the parameter gradients\n",
        "        optim.zero_grad()\n",
        "        \n",
        "        # Forward + backward + optimize\n",
        "        outputs = model(inputs)\n",
        "        loss = crit(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        # Print statistics\n",
        "        running_loss += loss.item()\n",
        "        \n",
        "        if batch_idx % log_interval == 0:\n",
        "            dataset_size = len(data_loader.dataset)\n",
        "            used_samples = batch_idx * len(inputs)\n",
        "            train_progress = 100. * batch_idx / len(data_loader)\n",
        "            avg_batch_loss = running_loss / log_interval\n",
        "            \n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch_num, used_samples, dataset_size, train_progress, avg_batch_loss\n",
        "            ))\n",
        "            \n",
        "            losses.append(avg_batch_loss)\n",
        "            running_loss = 0.0\n",
        "            \n",
        "    return model, losses\n",
        "     \n",
        "    \n",
        "def test(model, crit, data_loader, test_losses, test_accs):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    test_loss = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for data in data_loader:\n",
        "            # Get the inputs; data is a list of [inputs, labels]\n",
        "            inputs, targets = data\n",
        "\n",
        "            if use_cuda:\n",
        "              inputs, targets = inputs.cuda(), targets.cuda()\n",
        "            \n",
        "            # Forward + loss + correct\n",
        "            outputs = model(inputs)\n",
        "            test_loss += crit(outputs, targets).item()\n",
        "            \n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            correct += (predicted == targets).sum().item()\n",
        "          \n",
        "    dataset_size = len(data_loader.dataset)\n",
        "    test_loss /= dataset_size\n",
        "    acc = 100. * correct / dataset_size\n",
        "    \n",
        "    test_losses.append(test_loss)\n",
        "    test_accs.append(acc)\n",
        "    \n",
        "    print('\\nTest set: Avg. loss: {:.6f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n",
        "        test_loss, correct, dataset_size, acc\n",
        "    ))\n",
        "    \n",
        "    return test_losses, test_accs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s8o2aF5xoFTo"
      },
      "source": [
        "class MnistNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MnistNet, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 64, kernel_size=8, stride=2, padding=3)\n",
        "        self.relu1 = nn.ReLU(True)\n",
        "        self.conv2 = nn.Conv2d(64, 128, kernel_size=6, stride=2)\n",
        "        self.relu2 = nn.ReLU(True)\n",
        "        self.conv3 = nn.Conv2d(128, 128, kernel_size=5)\n",
        "        self.relu3 = nn.ReLU(True)\n",
        "        self.fc = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu1(self.conv1(x))\n",
        "        x = self.relu2(self.conv2(x))\n",
        "        x = self.relu3(self.conv3(x))\n",
        "        x = x.view(-1, 128)\n",
        "        x = self.fc(x)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tzkyXSShoGA9"
      },
      "source": [
        "# Create Model\n",
        "model = MnistNet()\n",
        "\n",
        "if use_cuda:\n",
        "    model.cuda()\n",
        "    print('Using ', torch.cuda.device_count(), ' GPU(s)')\n",
        "\n",
        "# Define Optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Start Training\n",
        "_, _, accs = train(num_epochs, model, optimizer, criterion, trainloader, testloader)\n",
        "\n",
        "print()\n",
        "print('Accuracies: {}'.format(accs))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aBz7uc7BoIVv"
      },
      "source": [
        "model.eval()\n",
        "\n",
        "if use_cuda:\n",
        "    model.cuda()\n",
        "    print('Using ', torch.cuda.device_count(), ' GPU(s)')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pWLCpus2oLA2"
      },
      "source": [
        "def test_final(model, data_loader):\n",
        "    _, accs = test(model, criterion, data_loader, [], [])\n",
        "    return accs[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JL0x1g_doNVT"
      },
      "source": [
        "test_acc = test_final(model, testloader); test_acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x677UWBvoQdv"
      },
      "source": [
        "model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fCsH-IwcoQ7l"
      },
      "source": [
        "layers = {\n",
        "    'relu1': model.relu1,\n",
        "    'relu2': model.relu2,\n",
        "    'relu3': model.relu3,\n",
        "    'fc': model.fc\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WX22viVHoS5L"
      },
      "source": [
        "def get_activations(dataloader, model, layers):\n",
        "    activations = {}\n",
        "    activations['activations'] = {}\n",
        "    activations['targets'] = None\n",
        "\n",
        "    for layer_name in layers:\n",
        "        print('## Fetching Activations from Layer {}'.format(layer_name))\n",
        "\n",
        "        # Get activations for the data\n",
        "        layer = layers[layer_name]\n",
        "        activations['activations'][layer_name], targets = get_activations_from_layer(dataloader, model, layer)\n",
        "\n",
        "        # Get the targets of that data\n",
        "        if targets is not None:\n",
        "            if activations['targets'] is not None:\n",
        "                np.testing.assert_array_equal(activations['targets'], targets)\n",
        "            else:\n",
        "                activations['targets'] = targets\n",
        "\n",
        "        print()\n",
        "\n",
        "    return activations\n",
        "\n",
        "def get_activations_from_layer(dataloader, model, layer):\n",
        "    activations = []\n",
        "    targets = []\n",
        "\n",
        "    # Define hook for fetching the activations\n",
        "    def hook(module, input, output):\n",
        "        layer_activations = output.squeeze().detach().cpu().numpy()\n",
        "\n",
        "        if len(layer_activations.shape) == 4:\n",
        "            layer_activations = layer_activations.reshape(layer_activations.shape[0], -1)\n",
        "        \n",
        "        activations.append(layer_activations)\n",
        "\n",
        "    handle = layer.register_forward_hook(hook)\n",
        "\n",
        "    # Fetch activations\n",
        "    for i, batch in enumerate(dataloader):\n",
        "        print('Processing Batch {}'.format(i))\n",
        "        \n",
        "\n",
        "        if use_cuda:\n",
        "            batch = [_batch.cuda() for _batch in batch]\n",
        "            #batch = batch.cuda()\n",
        "            \n",
        "\n",
        "        _ = model(batch[0])\n",
        "\n",
        "        if len(batch) > 1:\n",
        "          targets.append(batch[1].detach().cpu().numpy())\n",
        "\n",
        "    print(\"done!\")\n",
        "\n",
        "    # Remove hook\n",
        "    handle.remove()\n",
        "\n",
        "    # Return activations and targets\n",
        "    activations = np.concatenate(activations)\n",
        "\n",
        "    if targets:\n",
        "        targets = np.hstack(targets)\n",
        "    else:\n",
        "        None\n",
        "\n",
        "    return activations, targets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dnVplDg8oU86"
      },
      "source": [
        "acts = get_activations(calibloader, model, layers)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UrpVUohhoWuN"
      },
      "source": [
        "print('targets: {}'.format(acts['targets'].shape))\n",
        "print()\n",
        "\n",
        "for layer in layers:\n",
        "    print('## layer {}'.format(layer))\n",
        "    print('activations: {}'.format(acts['activations'][layer].shape))\n",
        "    print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mMrO8BA7oYtq"
      },
      "source": [
        "class NearestNeighbor:\n",
        "\n",
        "    class BACKEND(enum.Enum):\n",
        "        FALCONN = 1\n",
        "        FAISS = 2\n",
        "\n",
        "    def __init__(self, backend, dimension, neighbors, number_bits, nb_tables=None):\n",
        "        assert backend in NearestNeighbor.BACKEND\n",
        "\n",
        "        self._NEIGHBORS = neighbors\n",
        "        self._BACKEND = backend\n",
        "\n",
        "        if self._BACKEND is NearestNeighbor.BACKEND.FALCONN:\n",
        "            self._init_falconn(dimension, number_bits, nb_tables)\n",
        "        elif self._BACKEND is NearestNeighbor.BACKEND.FAISS:\n",
        "            self._init_faiss(dimension)\n",
        "        else:\n",
        "            raise NotImplementedError\n",
        "\n",
        "    def _init_falconn(self, dimension, number_bits, nb_tables):\n",
        "        assert nb_tables >= self._NEIGHBORS\n",
        "\n",
        "        # LSH parameters\n",
        "        params_cp = falconn.LSHConstructionParameters()\n",
        "        params_cp.dimension = dimension\n",
        "        params_cp.lsh_family = falconn.LSHFamily.CrossPolytope\n",
        "        params_cp.distance_function = falconn.DistanceFunction.EuclideanSquared\n",
        "        params_cp.l = nb_tables\n",
        "        params_cp.num_rotations = 2  # for dense set it to 1; for sparse data set it to 2\n",
        "        params_cp.seed = 5721840\n",
        "        params_cp.num_setup_threads = 0  # we want to use all the available threads to set up\n",
        "        params_cp.storage_hash_table = falconn.StorageHashTable.BitPackedFlatHashTable\n",
        "\n",
        "        # we build number_bits-bit hashes so that each table has\n",
        "        # 2^number_bits bins; a rule of thumb is to have the number\n",
        "        # of bins be the same order of magnitude as the number of data points\n",
        "        falconn.compute_number_of_hash_functions(number_bits, params_cp)\n",
        "        self._falconn_table = falconn.LSHIndex(params_cp)\n",
        "        self._falconn_query_object = None\n",
        "        self._FALCONN_NB_TABLES = nb_tables\n",
        "\n",
        "    def _init_faiss(self, dimension):\n",
        "        res = faiss.StandardGpuResources()\n",
        "        self._faiss_index = faiss.GpuIndexFlatL2(res, dimension)\n",
        "\n",
        "    def add(self, x):\n",
        "        if self._BACKEND is NearestNeighbor.BACKEND.FALCONN:\n",
        "            self._falconn_table.setup(x)\n",
        "        elif self._BACKEND is NearestNeighbor.BACKEND.FAISS:\n",
        "            self._faiss_index.add(x)\n",
        "        else:\n",
        "            raise NotImplementedError\n",
        "\n",
        "    def find_knns(self, x, output):\n",
        "        if self._BACKEND is NearestNeighbor.BACKEND.FALCONN:\n",
        "            return self._find_knns_falconn(x, output)\n",
        "        elif self._BACKEND is NearestNeighbor.BACKEND.FAISS:\n",
        "            return self._find_knns_faiss(x, output)\n",
        "        else:\n",
        "            raise NotImplementedError\n",
        "\n",
        "    def _find_knns_falconn(self, x, output):\n",
        "        # Late falconn query_object construction\n",
        "        # Since I suppose there might be an error\n",
        "        # if table.setup() will be called after\n",
        "        if self._falconn_query_object is None:\n",
        "            self._falconn_query_object = self._falconn_table.construct_query_object()\n",
        "            self._falconn_query_object.set_num_probes(self._FALCONN_NB_TABLES)\n",
        "\n",
        "        missing_indices = np.zeros(output.shape, dtype=np.bool)\n",
        "\n",
        "        for i in range(x.shape[0]):\n",
        "            query_res = self._falconn_query_object.find_k_nearest_neighbors(x[i], self._NEIGHBORS)\n",
        "\n",
        "            try:\n",
        "                output[i, :] = query_res\n",
        "            except:\n",
        "                # mark missing indices\n",
        "                missing_indices[i, len(query_res):] = True\n",
        "                output[i, :len(query_res)] = query_res\n",
        "\n",
        "        return missing_indices\n",
        "\n",
        "    def _find_knns_faiss(self, x, output):\n",
        "        neighbor_distance, neighbor_index = self._faiss_index.search(x, self._NEIGHBORS)\n",
        "\n",
        "        missing_indices = neighbor_distance == -1\n",
        "        d1 = neighbor_index.reshape(-1)\n",
        "\n",
        "        output.reshape(-1)[np.logical_not(missing_indices.flatten())] = d1[np.logical_not(missing_indices.flatten())]\n",
        "\n",
        "        return missing_indices\n",
        "\n",
        "\n",
        "class DkNN:\n",
        "\n",
        "    def __init__(self, model, nb_classes, neighbors, layers, trainloader, nearest_neighbor_backend, nb_tables=200, number_bits=17):\n",
        "        \"\"\"\n",
        "        Implementation of the DkNN algorithm, see https://arxiv.org/abs/1803.04765 for more details\n",
        "        :param model: model to be used\n",
        "        :param nb_classes: the number of classes in the task\n",
        "        :param neighbors: number of neighbors to find per layer\n",
        "        :param layers: a list of layer names to include in the DkNN\n",
        "        :param trainloader: data loader for the training data\n",
        "        :param nearest_neighbor_backend: falconn or faiss to be used for LSH\n",
        "        :param nb_tables: number of tables used by FALCONN to perform locality-sensitive hashing.\n",
        "        :param number_bits: number of hash bits used by LSH.\n",
        "        \"\"\"\n",
        "        print('---------- DkNN init')\n",
        "        print()\n",
        "\n",
        "        self.model = model\n",
        "        self.nb_classes = nb_classes\n",
        "        self.neighbors = neighbors\n",
        "        self.layers = layers\n",
        "        self.backend = nearest_neighbor_backend\n",
        "        self.nb_tables = nb_tables\n",
        "        self.number_bits = number_bits\n",
        "\n",
        "        self.nb_cali = -1\n",
        "        self.calibrated = False   \n",
        "\n",
        "        # Compute training data activations\n",
        "        activations = get_activations(trainloader, model, layers)\n",
        "        self.train_activations = activations['activations']\n",
        "        self.train_labels = activations['targets']\n",
        "\n",
        "        # Build locality-sensitive hashing tables for training representations\n",
        "        self.train_activations_lsh = copy.copy(self.train_activations)\n",
        "        self.init_lsh()\n",
        "\n",
        "    def init_lsh(self):\n",
        "        \"\"\"\n",
        "        Initializes locality-sensitive hashing with FALCONN to find nearest neighbors in training data\n",
        "        \"\"\"\n",
        "        self.query_objects = {} # contains the object that can be queried to find nearest neighbors at each layer\n",
        "        self.centers = {} # mean of training data representation per layer (that needs to be substracted before NearestNeighbor)\n",
        "\n",
        "        print(\"## Constructing the NearestNeighbor tables\")\n",
        "\n",
        "        for layer in self.layers:\n",
        "            print(\"Constructing table for {}\".format(layer))\n",
        "\n",
        "            # Normalize all the lenghts, since we care about the cosine similarity\n",
        "            self.train_activations_lsh[layer] /= np.linalg.norm(self.train_activations_lsh[layer], axis=1).reshape(-1, 1)\n",
        "\n",
        "            # Center the dataset and the queries: this improves the performance of LSH quite a bit\n",
        "            center = np.mean(self.train_activations_lsh[layer], axis=0)\n",
        "            self.train_activations_lsh[layer] -= center\n",
        "            self.centers[layer] = center\n",
        "\n",
        "            # Constructing nearest neighbor table\n",
        "            self.query_objects[layer] = NearestNeighbor(\n",
        "                backend=self.backend,\n",
        "                dimension=self.train_activations_lsh[layer].shape[1],\n",
        "                number_bits=self.number_bits,\n",
        "                neighbors=self.neighbors,\n",
        "                nb_tables=self.nb_tables,\n",
        "            )\n",
        "\n",
        "            self.query_objects[layer].add(self.train_activations_lsh[layer])\n",
        "\n",
        "        print(\"done!\")\n",
        "        print()\n",
        "\n",
        "\n",
        "    def calibrate(self, calibloader):\n",
        "        \"\"\"\n",
        "        Runs the DkNN on holdout data to calibrate the credibility metric\n",
        "        :param calibloader: data loader for the calibration loader\n",
        "        \"\"\"\n",
        "        print('---------- DkNN calibrate')\n",
        "        print()\n",
        "\n",
        "        # Compute calibration data activations\n",
        "        self.nb_cali = len(calibloader.dataset)\n",
        "        activations = get_activations(calibloader, self.model, self.layers)\n",
        "        self.cali_activations = activations['activations']\n",
        "        self.cali_labels = activations['targets']\n",
        "\n",
        "        print(\"## Starting calibration of DkNN\")\n",
        "\n",
        "        cali_knns_ind, cali_knns_labels = self.find_train_knns(self.cali_activations)\n",
        "        assert all([v.shape == (self.nb_cali, self.neighbors) for v in cali_knns_ind.values()])\n",
        "        assert all([v.shape == (self.nb_cali, self.neighbors) for v in cali_knns_labels.values()])\n",
        "\n",
        "        cali_knns_not_in_class = self.nonconformity(cali_knns_labels)\n",
        "        cali_knns_not_in_l = np.zeros(self.nb_cali, dtype=np.int32)\n",
        "\n",
        "        for i in range(self.nb_cali):\n",
        "            cali_knns_not_in_l[i] = cali_knns_not_in_class[i, self.cali_labels[i]]\n",
        "\n",
        "        cali_knns_not_in_l_sorted = np.sort(cali_knns_not_in_l)\n",
        "        self.cali_nonconformity = np.trim_zeros(cali_knns_not_in_l_sorted, trim='f')\n",
        "        self.nb_cali = self.cali_nonconformity.shape[0]\n",
        "        self.calibrated = True\n",
        "\n",
        "        print(\"DkNN calibration complete\")\n",
        "\n",
        "    def find_train_knns(self, data_activations):\n",
        "        \"\"\"\n",
        "        Given a data_activation dictionary that contains a np array with activations for each layer,\n",
        "        find the knns in the training data\n",
        "        \"\"\"\n",
        "        knns_ind = {}\n",
        "        knns_labels = {}\n",
        "\n",
        "        for layer in self.layers:\n",
        "            # Pre-process representations of data to normalize and remove training data mean\n",
        "            data_activations_layer = copy.copy(data_activations[layer])\n",
        "            nb_data = data_activations_layer.shape[0]\n",
        "            data_activations_layer /= np.linalg.norm(data_activations_layer, axis=1).reshape(-1, 1)\n",
        "            data_activations_layer -= self.centers[layer]\n",
        "\n",
        "            # Use FALCONN to find indices of nearest neighbors in training data\n",
        "            knns_ind[layer] = np.zeros((data_activations_layer.shape[0], self.neighbors), dtype=np.int32)\n",
        "            knn_errors = 0\n",
        "\n",
        "            knn_missing_indices = self.query_objects[layer].find_knns(data_activations_layer, knns_ind[layer])\n",
        "            knn_errors += knn_missing_indices.flatten().sum()\n",
        "\n",
        "            # Find labels of neighbors found in the training data\n",
        "            knns_labels[layer] = np.zeros((nb_data, self.neighbors), dtype=np.int32)\n",
        "\n",
        "            knns_labels[layer].reshape(-1)[\n",
        "                np.logical_not(knn_missing_indices.flatten())\n",
        "            ] = self.train_labels[\n",
        "                knns_ind[layer].reshape(-1)[np.logical_not(knn_missing_indices.flatten())]                    \n",
        "            ]\n",
        "\n",
        "        return knns_ind, knns_labels\n",
        "\n",
        "    def nonconformity(self, knns_labels):\n",
        "        \"\"\"\n",
        "        Given an dictionary of nb_data x nb_classes dimension, compute the nonconformity of\n",
        "        each candidate label for each data point: i.e. the number of knns whose label is\n",
        "        different from the candidate label\n",
        "        \"\"\"\n",
        "        nb_data = knns_labels[list(self.layers.keys())[0]].shape[0]\n",
        "        knns_not_in_class = np.zeros((nb_data, self.nb_classes), dtype=np.int32)\n",
        "\n",
        "        for i in range(nb_data):\n",
        "            # Compute number of nearest neighbors per class\n",
        "            knns_in_class = np.zeros((len(self.layers), self.nb_classes), dtype=np.int32)\n",
        "\n",
        "            for layer_id, layer in enumerate(self.layers):\n",
        "                knns_in_class[layer_id, :] = np.bincount(knns_labels[layer][i], minlength=self.nb_classes)\n",
        "\n",
        "            # Compute number of knns in other class than class_id\n",
        "            for class_id in range(self.nb_classes):\n",
        "                knns_not_in_class[i, class_id] = np.sum(knns_in_class) - np.sum(knns_in_class[:, class_id])\n",
        "\n",
        "        return knns_not_in_class\n",
        "\n",
        "    def fprop(self, testloader):\n",
        "        \"\"\"\n",
        "        Performs a forward pass through the DkNN on an numpy array of data\n",
        "        \"\"\"\n",
        "        print('---------- DkNN predict')\n",
        "        print()\n",
        "\n",
        "        if not self.calibrated:\n",
        "            raise ValueError(\"DkNN needs to be calibrated by calling DkNNModel.calibrate method once before inferring\")\n",
        "\n",
        "        # Compute test data activations\n",
        "        activations = get_activations(testloader, self.model, self.layers)\n",
        "        data_activations = activations['activations']\n",
        "        _, knns_labels = self.find_train_knns(data_activations)\n",
        "\n",
        "        # Calculate nonconformity\n",
        "        knns_not_in_class = self.nonconformity(knns_labels)\n",
        "        print('Nonconformity calculated')\n",
        "\n",
        "        # Create predictions, confidence and credibility\n",
        "        _, _, creds = self.preds_conf_cred(knns_not_in_class)\n",
        "        print('Predictions created')\n",
        "\n",
        "        return creds, activations['targets']\n",
        "\n",
        "    def preds_conf_cred(self, knns_not_in_class):\n",
        "        \"\"\"\n",
        "        Given an array of nb_data x nb_classes dimensions, use conformal prediction to compute\n",
        "        the DkNN's prediction, confidence and credibility\n",
        "        \"\"\"\n",
        "        nb_data = knns_not_in_class.shape[0]\n",
        "        preds_knn = np.zeros(nb_data, dtype=np.int32)\n",
        "        confs = np.zeros((nb_data, self.nb_classes), dtype=np.float32)\n",
        "        creds = np.zeros((nb_data, self.nb_classes), dtype=np.float32)\n",
        "\n",
        "        for i in range(nb_data):\n",
        "            # p-value of test input for each class\n",
        "            p_value = np.zeros(self.nb_classes, dtype=np.float32)\n",
        "\n",
        "            for class_id in range(self.nb_classes):\n",
        "                # p-value of (test point, candidate label)\n",
        "                p_value[class_id] = (float(self.nb_cali) - bisect_left(self.cali_nonconformity, knns_not_in_class[i, class_id])) / float(self.nb_cali)\n",
        "\n",
        "            preds_knn[i] = np.argmax(p_value)\n",
        "            confs[i, preds_knn[i]] = 1. - np.sort(p_value)[-2]\n",
        "            creds[i, preds_knn[i]] = p_value[preds_knn[i]]\n",
        "\n",
        "        return preds_knn, confs, creds\n",
        "\n",
        "\n",
        "def plot_reliability_diagram(confidence, labels):\n",
        "    \"\"\"\n",
        "    Takes in confidence values (e.g. output of softmax or DkNN confidences) for\n",
        "    predictions and correct labels for the data, plots a reliability diagram\n",
        "    :param confidence: nb_samples x nb_classes with confidence scores\n",
        "    :param labels: targets\n",
        "    \"\"\"\n",
        "    assert len(confidence.shape) == 2\n",
        "    assert len(labels.shape) == 1\n",
        "    assert confidence.shape[0] == labels.shape[0]\n",
        "\n",
        "    if confidence.max() <= 1.:\n",
        "        # confidence array is output of softmax\n",
        "        bins_start = [b / 10. for b in range(0, 10)]\n",
        "        bins_end = [b / 10. for b in range(1, 11)]\n",
        "        bins_center = [(b + .5) / 10. for b in range(0, 10)]\n",
        "        preds_conf = np.max(confidence, axis=1)\n",
        "        preds_l = np.argmax(confidence, axis=1)\n",
        "    else:\n",
        "        raise ValueError('Confidence values go above 1')\n",
        "\n",
        "    print(preds_conf.shape, preds_l.shape)\n",
        "\n",
        "    # Create var for reliability diagram (Will contain mean accuracies for each bin)\n",
        "    reliability_diag = []\n",
        "    num_points = []  # keeps the number of points in each bar\n",
        "\n",
        "    # Find average accuracy per confidence bin\n",
        "    for bin_start, bin_end in zip(bins_start, bins_end):\n",
        "        above = preds_conf >= bin_start\n",
        "\n",
        "        if bin_end == 1.:\n",
        "            below = preds_conf <= bin_end\n",
        "        else:\n",
        "            below = preds_conf < bin_end\n",
        "\n",
        "        mask = np.multiply(above, below)\n",
        "        num_points.append(np.sum(mask))\n",
        "\n",
        "        bin_mean_acc = max(0, np.mean(preds_l[mask] == labels[mask]))\n",
        "        reliability_diag.append(bin_mean_acc)\n",
        "\n",
        "    # Plot diagram\n",
        "    assert len(reliability_diag) == len(bins_center)\n",
        "    #print(reliability_diag)\n",
        "    #print(bins_center)\n",
        "    #print(num_points)\n",
        "\n",
        "    fig, ax1 = plt.subplots()\n",
        "    _ = ax1.bar(bins_center, reliability_diag, width=.1, alpha=0.8, edgecolor = \"black\")\n",
        "    plt.xlim([0, 1.])\n",
        "    ax1.set_ylim([0, 1.])\n",
        "\n",
        "    ax2 = ax1.twinx()\n",
        "\n",
        "    #print(sum(num_points))\n",
        "\n",
        "    ax2.plot(bins_center, num_points, color='r', linestyle='-', linewidth=7.0)\n",
        "    ax2.set_ylabel('Number of points in the data', fontsize=16, color='r')\n",
        "\n",
        "    if len(np.argwhere(confidence[0] != 0.)) == 1:\n",
        "        # This is a DkNN diagram\n",
        "        ax1.set_xlabel('Prediction Credibility', fontsize=16)\n",
        "    else:\n",
        "        # This is a softmax diagram\n",
        "        ax1.set_xlabel('Prediction Confidence', fontsize=16)\n",
        "\n",
        "    ax1.set_ylabel('Prediction Accuracy', fontsize=16)\n",
        "    ax1.tick_params(axis='both', labelsize=14)\n",
        "    ax2.tick_params(axis='both', labelsize=14, colors='r')\n",
        "    fig.tight_layout()\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fMsMTopeoeGj"
      },
      "source": [
        "# Create training data loader for DkNN\n",
        "trainloader_dknn = torch.utils.data.DataLoader(\n",
        "    trainset, batch_size=batch_size_train, shuffle=False, num_workers=2\n",
        ")\n",
        "\n",
        "# Initialize DkNN\n",
        "nb_classes_mnist = 10\n",
        "dknn = DkNN(model, nb_classes_mnist, neighbors, layers, trainloader_dknn, NearestNeighbor.BACKEND.FALCONN)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FKwOONB0ogCh"
      },
      "source": [
        "dknn.calibrate(calibloader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Np8XnevtojEs"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "#X = np.load('poisoned_data_X.npy')\n",
        "\n",
        "#Y = np.load('poisoned_data_Y.npy')\n",
        "\n",
        "#Y = np.array([1, 9, 1, 1, 1, 1, 1, 9, 9, 1, 1, 9, 9, 9, 1, 9, 1, 1, 9, 1, 1, 9,\n",
        "#       1, 9, 9, 9, 9, 9, 9, 1, 9, 1, 1, 9, 1, 9, 9, 1, 1, 1, 9, 9, 1, 1,\n",
        "#       9, 1, 1, 9, 1, 9])\n",
        "X = pois_points_ds.X.get_data()\n",
        "Y = pois_points_ds.Y.get_data()\n",
        "\n",
        "Y = np.where(Y==0,9,1)\n",
        "\n",
        "print(Y)\n",
        "print(X)\n",
        "#Y = np.array([1, 9, 1, 1, 1, 1, 1, 9, 9, 1, 1, 9, 9, 9, 1])\n",
        "test_X = torch.Tensor(X)\n",
        "test_Y = torch.Tensor(Y)\n",
        "\n",
        "test_X = test_X.reshape((-1,1,28,28))\n",
        "ds = TensorDataset(test_X, test_Y)\n",
        "\n",
        "tss = torch.utils.data.DataLoader(ds, batch_size=batch_size_test, shuffle=False, num_workers=2)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hK8nhNflooLi"
      },
      "source": [
        "dknn_preds_testset, test_targets = dknn.fprop(tss)\n",
        "\n",
        "#dknn_preds_testset1, test_targets1 = dknn.fprop(testloader)\n",
        "\n",
        "\n",
        "print()\n",
        "print('---------------------------------------------')\n",
        "print('preds:   {}'.format(dknn_preds_testset.shape))\n",
        "print('targets: {}'.format(test_targets.shape))\n",
        "print(dknn_preds_testset)\n",
        "print(test_targets)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ybdvVomtotw-"
      },
      "source": [
        "with warnings.catch_warnings():\n",
        "    warnings.simplefilter(\"ignore\", category=RuntimeWarning)\n",
        "    plot_reliability_diagram(dknn_preds_testset, test_targets)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VCUEz_MtouXU"
      },
      "source": [
        "dknn_preds_advset, adv_targets = dknn.fprop(tss)\n",
        "\n",
        "print()\n",
        "print('---------------------------------------------')\n",
        "print('preds:   {}'.format(dknn_preds_advset.shape))\n",
        "print('targets: {}'.format(test_targets.shape))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gacGranNo0EL"
      },
      "source": [
        "with warnings.catch_warnings():\n",
        "    warnings.simplefilter(\"ignore\", category=RuntimeWarning)\n",
        "    plot_reliability_diagram(dknn_preds_advset, test_targets)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}